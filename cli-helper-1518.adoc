# TXC 2024 - Lab 1518 Helper Commands
//++++
//<link rel="stylesheet"  href="http://cdnjs.cloudflare.com/ajax/libs/font-awesome/3.1.0/css/font-awesome.min.css">
//++++
:icons: font
:source-language: shell
:numbered:
// Activate experimental attribute for Keyboard Shortcut keys
:experimental:
:source-highlighter: pygments
:sectnums:
:sectnumlevels: 6
:toc: left
:toclevels: 4

## Introduction

### About this Lab

### High Level Architecture

### Accessing your Lab environment

Accessing your Lab Workstation (the physical laptop) via ssh:

[source, shell]
----
export USERNAME={your_username}
export USERIP={your_public_ip}
chmod 0600 ~/Downloads/ssh_private_key.pem
ssh -i ~/Downloads/ssh_private_key.pem -p 2223 ${USERNAME}@${USERIP}
----

Accessing your Workstation (the physical laptop) using a browser via CLI

**Windows:**

[source, shell]
----
cmd.exe
set USERIP={your_public_ip}
start https://${your_public_ip}
----

**MacOS/Linux:**

[source, shell]
----
set USERIP={your_public_ip}
open -a "Google Chrome" "https://${USERIP}"
open -a "Safari" "https://${USERIP}"
----

## IBM Storage Ceph configuration using the Dashboard

### Connect to the Jump Workstation Desktop

### Set the admin password

### Expand the cluster

### Add Node Labels

[source, shell]
----
grep node1 /etc/hosts | awk '{ print $2 }' | sed -e 's/node1-/node[2-4]-/g'
----

### Add Drives/OSDs to the Cluster

### Accept Grafana Self-Signed Cert to view performance details

.Generate URL for Certificate exception
[source, shell]
----
grep node1 /etc/hosts | awk '{ print $2 }'| sed -e 's#ceph-node1-#https://ceph-node1-#g' | sed -e 's/$/:3000/'
----

## Ceph Object Gateway(RGW) configuration

### RGW service creation

.Workaround for SSL restart RGW to enable SSL
[source, shell]
----
ssh ceph-node1 sudo ceph orch restart rgw.rgwsrv
----

#### Data Pool creation

#### Create an Object Storage user account

Before you create Object Store User Account check all PGs are in ok state

.Check PG Status
[source, shell]
----
ssh ceph-node1 sudo ceph pg stat
----

#### Create a bucket

### Bucket Access

#### Install AWS CLI

[source, shell]
----
sudo dnf install awscli -y
----

#### Configure AWS CLI

.Extract Autogenerated access and secret keys
[source, shell]
----
ssh ceph-node1 sudo radosgw-admin user info --uid=labuser | jq -r '.keys[0] | .access_key, .secret_key'
----

#### Using AWS Client
.Configure keys to use
[source, shell]
----
export AKEY=$(ssh ceph-node1 sudo radosgw-admin user info --uid=labuser | jq -r '.keys[0].access_key');echo $AKEY
export SKEY=$(ssh ceph-node1 sudo radosgw-admin user info --uid=labuser | jq -r '.keys[0].secret_key');echo $SKEY
aws configure set aws_access_key_id $AKEY --profile labuser 
aws configure set aws_secret_access_key $SKEY --profile labuser 
----

.Configure endpoint specifics
[source, shell]
----
aws configure set endpoint_url https://ceph-node3 --profile labuser 
aws configure set region multizg --profile labuser
aws configure set ca_bundle ${HOME}/rootCA.pem  --profile labuser
----

Using AWS CLI

.List buckets
[source, shell]
----
aws --profile labuser s3 ls
----

.Create bucket
[source, shell]
----
aws --profile labuser s3api create-bucket --bucket s3-bucket-2
aws --profile labuser s3 ls
----

.Upload Object
[source, shell]
----
truncate -s 10M 10MB.bin
aws --profile labuser s3 cp 10MB.bin s3://s3-bucket-1/10MB.bin
----

.Get Bucket Listing
[source, shell]
----
aws --profile labuser s3 ls s3://s3-bucket-1 
aws --profile labuser s3 cp s3://s3-bucket-1/10MB.bin GET-10MB.bin
echo $(openssl dgst -md5 ./10MB.bin | awk '{print $2}');echo $(openssl dgst -md5 ./GET-10MB.bin | awk '{print $2}')
----

### Multitenancy, Quotas and Rate Limiting

#### Quotas

.Quotas
[source, shell]
----
truncate -s 1M ./1MB.bin
for i in {1..12} ; do aws --profile labuser s3 cp ./1MB.bin s3://s3-bucket-2/1MB-${i}.bin ; done
----

.Checking if quotas are enabled
[source, shell]
----
ssh ceph-node1 sudo radosgw-admin user info --uid=labuser | jq '.user_quota.enabled'
----

.Disable Quota once you finished the exercise
[source, shell]
----
ssh ceph-node1 sudo radosgw-admin quota disable --quota-scope=user --uid=labuser
----

.Check Quotas era disabled
[source, shell]
----
ssh ceph-node1 sudo radosgw-admin user info --uid=labuser | jq '.user_quota.enabled'
----

#### Object Gateway Rate Limiting

.Set Rate Limiting
[source, shell]
----
ssh ceph-node1 sudo radosgw-admin ratelimit set --ratelimit-scope=user --uid=labuser --max-read-ops=3 
----

.Enable Rate Limiting
[source, shell]
----
ssh ceph-node1 sudo radosgw-admin ratelimit enable --ratelimit-scope=user --uid=labuser 
----

.Check Rate Limit Settings
[source, shell]
----
ssh ceph-node1 sudo radosgw-admin ratelimit get --ratelimit-scope=user  --uid=labuser
----

.Upload a test object
[source, shell]
----
aws --profile labuser s3 cp 1MB.bin s3://s3-bucket-1/1MB.bin
----

.Test Rate Limit is in effect
[source, shell]
----
for i in {1..5} ; do aws --profile labuser s3api head-object --bucket s3-bucket-1 --key 1MB.bin | grep ETag ; done
----

.Check Put Not affected
[source, shell]
----
aws --profile labuser s3 cp 1MB.bin s3://s3-bucket-1/1MB-2.bin
----

.Disable once the exercise has finished
[source, shell]
----
ssh ceph-node1 sudo radosgw-admin ratelimit disable --ratelimit-scope=user --uid=labuser
----

### Object Tiering, S3 Lifecycle and Storage Classes

#### Ceph Object Gateway Storage Classes and Life Cycle policy

#### Configure a new Storage Class called STANDARD_IA

.1 Create Data Pool
[source, shell]
----
ssh ceph-node1 sudo ceph osd pool create zone1.rgw.hdd.storage.class.buckets.data 32 32
ssh ceph-node1 sudo ceph osd pool application enable zone1.rgw.hdd.storage.class.buckets.data rgw
----

.2 Check Current Storage Class Settings
[source, shell]
----
ssh ceph-node1 sudo radosgw-admin zone get | jq .placement_pools
----

.3 Create Storage Class for Zonegroup
[source, shell]
----
ssh ceph-node1 sudo radosgw-admin zonegroup placement add --rgw-zonegroup multizg --placement-id default-placement --storage-class STANDARD_IA
----

.4 Add Data Pool to Zone
[source, shell]
----
ssh ceph-node1 sudo radosgw-admin zone placement add --rgw-zone zone1 --placement-id default-placement --storage-class STANDARD_IA --data-pool zone1.rgw.hdd.storage.class.buckets.data --compression lz4
----

.5 Commit Changes
[source, shell]
----
ssh ceph-node1 sudo radosgw-admin period update --commit
----

### Configure a transition Life Cycle Policy

.1	Create the new bucket called “bucketpolicy”
[source, shell]
----
aws --profile labuser s3 mb s3://bucketpolicy
----

.2 Create the Life Cycle Policy JSON file
[source, shell]
----
cat >lc_transition_rule <<EOF 
{ 
    "Rules": [ 
        { 
            "ID": "TransitionRule", 
            "Filter": { 
                "Prefix": "" 
            }, 
            "Status": "Enabled", 
            "Transitions": [ 
                { 
                    "Days": 1, 
                    "StorageClass": "STANDARD_IA" 
                } 
            ] 
        } 
    ] 
} 
EOF 
----

.3 Apply the Life Cycle Policy Json to the newly created bucket “bucketpolicy”
[source, shell]
----
aws --profile labuser s3api put-bucket-lifecycle-configuration --lifecycle-configuration file://lc_transition_rule --bucket bucketpolicy
aws --profile labuser s3api get-bucket-lifecycle-configuration --bucket bucketpolicy 
----

.4 Upload a test file to our bucket
[source, shell]
----
aws --profile labuser s3 cp 1MB.bin s3://bucketpolicy/
----

.5 Check that at the moment our STANDARD_IA pool is empty
[source, shell]
----
ssh ceph-node1 sudo ceph df -f json | jq '.pools[] | select(.name=="zone1.rgw.hdd.storage.class.buckets.data")'
----

.6 Use lc debug interval option to test if the LC rule is working
[source, shell]
----
ssh ceph-node1 sudo ceph config set client.rgw rgw_lc_debug_interval 60
ssh ceph-node1 sudo ceph orch restart rgw.rgwsrv
----

It will take around three minutes until the object 1MB.bin gets transitioned into the HDD storage class

.7 Check that the object has been transitioned 
[source, shell]
----
aws --profile labuser s3api head-object --bucket bucketpolicy --key 1MB.bin 
----

.8 Check Data is now in the new "Slow" pool
[source, shell]
----
ssh ceph-node1 sudo ceph df | grep -E '(OBJECTS|zone1.rgw.hdd.storage.class.buckets.data)'
----

.9 Direct Upload to Storage Class
[source, shell]
----
aws --profile labuser s3 cp 10MB.bin s3://bucketpolicy --storage-class STANDARD_IA
----

### Security. Configure Audit Logs

.Enable Ops Log
[source, shell]
----
ssh ceph-node1 sudo ceph config set client.rgw rgw_enable_ops_log true 
ssh ceph-node1 sudo ceph config set client.rgw rgw_ops_log_rados false 
----

.Set Log File Path
[source, shell]
----
ssh ceph-node1 sudo ceph config set client.rgw rgw_ops_log_file_path /var/log/ceph/audit_rgw.log
----

.Check RGW Servie and Restart
[source, shell]
----
ssh ceph-node1 sudo ceph orch ls | grep rgw
ssh ceph-node1 sudo ceph orch  restart  rgw.rgwsrv
----

### Secutiry. Bucket Sharing

At this point you need to have created the demouser from the UI

.3 Configure AWS CLI with new demo user
[source, shell]
----
export AKEY=$(ssh ceph-node1 sudo radosgw-admin user info --uid=demouser | jq -r '.keys[0].access_key');echo $AKEY
export SKEY=$(ssh ceph-node1 sudo radosgw-admin user info --uid=demouser | jq -r '.keys[0].secret_key');echo $SKEY
aws configure set aws_access_key_id $AKEY --profile demouser
aws configure set aws_secret_access_key $SKEY --profile demouser
aws configure set endpoint_url https://ceph-node3 --profile demouser
aws configure set region multizg --profile demouser
aws configure set ca_bundle $HOME/rootCA.pem  --profile demouser
----

.4 Check access with original labuser
[source, shell]
----
aws --profile labuser s3 ls s3://demobucket 
aws --profile labuser s3 cp /etc/hosts s3://demobucket/hosts
----

.5 Check access with new demouser
[source, shell]
----
aws --profile demouser s3 ls s3://demobucket
----

.6 Policy with GET permission
[source]
----
{
  "Id": "Policy1722543216639",
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "Stmt1722543108108",
      "Action": [
        "s3:ListBucket"
      ],
      "Effect": "Allow",
      "Resource": "arn:aws:s3:::demobucket",
      "Principal": {
        "AWS": [
          "arn:aws:iam:::user/demouser"
        ]
      }
    },
    {
      "Sid": "Stmt1722543202837",
      "Action": [
        "s3:GetObject"
      ],
      "Effect": "Allow",
      "Resource": "arn:aws:s3:::demobucket/*",
      "Principal": {
        "AWS": [
          "arn:aws:iam:::user/demouser"
        ]
      }
    }
  ]
}
----

.GetObjet + PutObject Policy

.8 Check Access After applying policy
[source, shell]
----
aws --profile labuser s3 cp /etc/hosts s3://demobucket/hosts 
aws --profile demouser s3 ls s3://demobucket

aws --profile demouser s3 cp s3://demobucket/hosts /tmp
cat /tmp/hosts 
----

.9 Check Write access not working
[source, shell]
----
aws --profile demouser s3 cp 1MB.bin s3://demobucket/
----

.10 Modified Policy with PUT permission
[source, shell]
----
{
  "Id": "Policy1722543216639",
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "Stmt1722543108108",
      "Action": [
        "s3:ListBucket"
      ],
      "Effect": "Allow",
      "Resource": "arn:aws:s3:::demobucket",
      "Principal": {
        "AWS": [
          "arn:aws:iam:::user/demouser"
        ]
      }
    },
    {
      "Sid": "Stmt1722543202837",
      "Action": [
        "s3:GetObject",
        "s3:PutObject"
      ],
      "Effect": "Allow",
      "Resource": "arn:aws:s3:::demobucket/*",
      "Principal": {
        "AWS": [
          "arn:aws:iam:::user/demouser"
        ]
      }
    }
  ]
}
----

.12 Check Access after applying the modified policy
[source, shell]
----
aws --profile demouser s3 ls s3://demobucket/
----

.13 Check still no permission to delete objects
[source, shell]
----
aws --profile demouser s3 rm s3://demobucket/1MB.bin
----

